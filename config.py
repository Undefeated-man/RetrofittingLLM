device = "gpu"
precision = "bf16"
log_dir = "logs"
config_pth = "./config/tinyllama_opt.json" # "./config/feedback_gpt2.json"  # None
evalute_after_train = False

# pretrained model config
# batch_size = 24
# eval_batch_size = 32
# dataset = "cerebras/SlimPajama-627B"
# eval_steps = 500
# learning_rate = 1e-3
# warmup_steps = 1000
# grad_clip = 1.0
# mix_precision = True
# grad_accumulation_steps = 10
# preload = False
# weight_decay = 1e-4
# load_best_model_at_end = True
# metric_for_best_model = "eval_perplexity"
# greater_is_better = False
# total_steps = 40000
# max_input_length = 1024
# eval_sample = 1000
# save_steps = 500
# model_name = "opt-llama"
# checkpoint_dir = None
# # checkpoint_dir = "/workspace/RetrofittingLLM/results/checkpoint-500/"
# eval_dataset = "CogComp/trec" # "cais/mmlu"
# tuning_mode = False

# finetuning config
tuning_mode = True
tuning_set = "stanfordnlp/coqa"  # "cais/mmlu"
eval_dataset = "stanfordnlp/coqa" # "cais/mmlu"
lora_r = 12
lora_alpha = 16
batch_size = 8
eval_batch_size = 24
epochs = 10
eval_steps = 50
learning_rate = 2e-4
warmup_steps = 600
grad_clip = 1.0
mix_precision = True
grad_accumulation_steps = 10
preload = False
weight_decay = 1e-4
load_best_model_at_end = True
metric_for_best_model = None
greater_is_better = False
total_steps = 6000
max_input_length = 1024
eval_sample = 1000000
model_name = "opt-llama"
checkpoint_dir = "/workspace/RetrofittingLLM/results/checkpoint-200/"
save_steps = 100