device = "gpu"
precision = "bf16"
log_dir = "logs"
config_pth = "./config/gpt2.json" # "./config/feedback_gpt2.json" # "./config/tinyllama_opt.json"  # None
evalute_after_train = False

# pretrained model config
batch_size = 24
eval_batch_size = 32
dataset = "cerebras/SlimPajama-627B"
eval_steps = 50
learning_rate = 2e-2
grad_clip = 1.0
mix_precision = True
grad_accumulation_steps = 10
preload = False
weight_decay = 1e-4
load_best_model_at_end = True
metric_for_best_model = "eval_perplexity"
greater_is_better = False
total_steps = 6000
warmup_steps = total_steps//10 if total_steps > 5000 else 500
max_input_length = 1024
eval_sample = 1000
save_steps = 50
model_name = "feedback-gpt2"
checkpoint_dir = None
eval_dataset = None
# checkpoint_dir = "/workspace/RetrofittingLLM/results/checkpoint-5000/"
# eval_dataset = "CogComp/trec" # "cais/mmlu"
tuning_mode = False

# finetuning config
# tuning_mode = True
# tuning_set = "stanfordnlp/coqa"  # "cais/mmlu"
# eval_dataset = "stanfordnlp/coqa" # "cais/mmlu"
# lora_r = 12
# lora_alpha = 16
# batch_size = 24
# eval_batch_size = 32
# epochs = 10
# eval_steps = 50
# learning_rate = 2e-4
# grad_clip = 1.0
# mix_precision = True
# grad_accumulation_steps = 10
# preload = False
# weight_decay = 1e-4
# load_best_model_at_end = True
# metric_for_best_model = None
# greater_is_better = False
# total_steps = 2000
# warmup_steps = total_steps//10
# max_input_length = 1024
# eval_sample = 50
# model_name = "gpt2"
# # checkpoint_dir = None
# # checkpoint_dir = "/workspace/RetrofittingLLM/results/checkpoint-1550/"
# save_steps = 100