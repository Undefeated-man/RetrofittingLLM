device = "gpu"
precision = "bf16"
log_dir = "logs"
config_pth = "./config/gpt2.json" # "./config/feedback_gpt2.json" # "./config/tinyllama.json"  # None
evalute_after_train = False

# pretrained model config
# batch_size = 2
# eval_batch_size = 4
# dataset = "cerebras/SlimPajama-627B"
# eval_steps = 50
# learning_rate = 5e-6
# grad_clip = 1.0
# mix_precision = True
# grad_accumulation_steps = 10
# preload = False
# weight_decay = 1e-4
# load_best_model_at_end = True
# metric_for_best_model = "eval_perplexity"
# greater_is_better = False
# total_steps = 3000
# warmup_steps = total_steps//10 if total_steps < 2000 else 200
# max_input_length = 1024
# eval_sample = 1000
# save_steps = 50
# model_name = "gemma"
# checkpoint_dir = None
# eval_dataset = None
# # checkpoint_dir = "/workspace/RetrofittingLLM/results/checkpoint-5000/"
# # eval_dataset = "CogComp/trec" # "cais/mmlu"
# tuning_mode = False

# finetuning config
tuning_mode = True
tuning_set = "EleutherAI/lambada_openai"  #  "stanfordnlp/coqa" # "cais/mmlu"
eval_dataset = "EleutherAI/lambada_openai" # "stanfordnlp/coqa" # "cais/mmlu"
lora_r = 12
lora_alpha = 16
batch_size = 1
eval_batch_size = 1
epochs = 10
eval_steps = 50
learning_rate = 1e-5
grad_clip = 1.0
mix_precision = True
grad_accumulation_steps = 10
preload = False
weight_decay = 1e-4
load_best_model_at_end = True
metric_for_best_model = "eval_accuracy"
greater_is_better = True
total_steps = 3000
warmup_steps = total_steps//10 if total_steps < 2000 else 200
max_input_length = 256
eval_sample = 50
model_name = "gemma"
checkpoint_dir = None
# checkpoint_dir = "/workspace/RetrofittingLLM/results/checkpoint-1550/"
save_steps = 100